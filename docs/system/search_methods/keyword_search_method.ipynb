{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/krixik-ai/krixik-docs/blob/main/docs/system/search_methods/keyword_search_method.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "remove_cell",
     "remove_output"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCESS: You are now authenticated.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import importlib\n",
    "from pathlib import Path\n",
    "\n",
    "# demo setup - including secrets instantiation, requirements installation, and path setting\n",
    "if os.getenv(\"COLAB_RELEASE_TAG\"):\n",
    "    # if running this notebook in collab - make sure to enter your secrets\n",
    "    MY_API_KEY = \"YOUR_API_KEY_HERE\"\n",
    "    MY_API_URL = \"YOUR_API_URL_HERE\"\n",
    "\n",
    "    # if running this notebook on collab - install requirements and pull required subdirectories\n",
    "    # install krixik python client\n",
    "    !pip install krixik\n",
    "\n",
    "    # install github clone - allows for easy cloning of subdirectories from docs repo: https://github.com/krixik-ai/krixik-docs\n",
    "    !pip install github-clone\n",
    "\n",
    "    # clone datasets\n",
    "    if not Path(\"data\").is_dir():\n",
    "        !ghclone https://github.com/krixik-ai/krixik-docs/tree/main/data\n",
    "    else:\n",
    "        print(\"docs datasets already cloned!\")\n",
    "\n",
    "    # define data dir\n",
    "    data_dir = \"./data/\"\n",
    "\n",
    "    # create output dir\n",
    "    from pathlib import Path\n",
    "\n",
    "    Path(data_dir + \"/output\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # pull utilities\n",
    "    if not Path(\"utilities\").is_dir():\n",
    "        !ghclone https://github.com/krixik-ai/krixik-docs/tree/main/utilities\n",
    "    else:\n",
    "        print(\"docs utilities already cloned!\")\n",
    "else:\n",
    "    # if running local pull of docs - set paths relative to local docs structure\n",
    "    # import utilities\n",
    "    sys.path.append(\"../../../\")\n",
    "\n",
    "    # define data_dir\n",
    "    data_dir = \"../../../data/\"\n",
    "\n",
    "    # if running this notebook locally from krixik docs repo - load secrets from a .env placed at the base of the docs repo\n",
    "    from dotenv import load_dotenv\n",
    "\n",
    "    load_dotenv(\"../../../.env\")\n",
    "\n",
    "    MY_API_KEY = os.getenv(\"MY_API_KEY\")\n",
    "    MY_API_URL = os.getenv(\"MY_API_URL\")\n",
    "\n",
    "\n",
    "# load in reset\n",
    "reset = importlib.import_module(\"utilities.reset\")\n",
    "reset_pipeline = reset.reset_pipeline\n",
    "\n",
    "\n",
    "# import krixik and initialize it with your personal secrets\n",
    "from krixik import krixik\n",
    "\n",
    "krixik.init(api_key=MY_API_KEY, api_url=MY_API_URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The `keyword_search` Method\n",
    "\n",
    "Krixik's `keyword_search` method enables keyword search on documents processed through certain pipelines. Keyword search is something internet users are long familiar with: a string of words is submitted as a query, and the search returns any and every instance of any of those words. Contrast this to [semantic search](semantic_search_method.md).\n",
    "\n",
    "The `keyword_search` method can only be used with pipelines ending with the [`keyword-db`](../../modules/database_modules/keyword-db_module.md) module.\n",
    "\n",
    "This overview of the `keyword_search` method is divided into the following sections:\n",
    "\n",
    "- [keyword_search Method Arguments](#keyword_search-method-arguments)\n",
    "- [Example Pipeline Setup and File Processing](#example-pipeline-setup-and-file-processing)\n",
    "- [Example Keyword Searches](#example-keyword-searches)\n",
    "- [Output Size Cap](#output-size-cap)\n",
    "- [Stop Words](#stop-words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `keyword_search` Method Arguments\n",
    "\n",
    "The `keyword_search` method takes one required argument and at least one of several optional arguments. The required argument is:\n",
    "\n",
    "- `query` (str) - A string that contains one or more keywords separated by spaces or hyphens that will be individually searched for across the target document.\n",
    "\n",
    "The optional arguments are the same arguments that the [`list`](../file_system/list_method.md) method takes—both metadata and timestamp bookends—so please take a moment to [review them here](../file_system/list_method.md). As with the [`list`](../file_system/list_method.md) method, you can `keyword_search` across several files at the same time because all metadata arguments are submitted to the `keyword_search` method in list format. All optional argument elements are the same as for the [`list`](../file_system/list_method.md) method, including the wildcard operator and the global root.\n",
    "\n",
    "If none of these optional arguments is present, the `keyword_search` method will not work because there will be nothing to search through.\n",
    "\n",
    "Like the [`list`](../file_system/list_method.md) method, the `keyword_search` method also accepts the optional `max_files` and `sort_order` arguments, though their function changes a bit:\n",
    "\n",
    "- `max_files` specifies up to how many files should be searched through.\n",
    "\n",
    "- `sort_order` here takes two possible values: 'ascending' and descending'. This determines what order searched-through files are returned in (in terms of their creation timestamp), but keyword results within each file are displayed in order of appearance. Default is 'descending'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Pipeline Setup and File Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this document's examples we will use a pipeline consisting of a single [`keyword-db`](../../modules/database_modules/keyword-db_module.md) module. This is the basic keyword search pipeline. We use the [`.create_pipeline`](../pipeline_creation/create_pipeline.md) method to instantiate the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the basic keyword search pipeline\n",
    "pipeline = krixik.create_pipeline(name=\"keyword_search_method_1_keyword-db\", module_chain=[\"keyword-db\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pipeline ready, we'll [`process`](../parameters_processing_files_through_pipelines/process_method.md) a few text files through it so we have something to search across. Let's use the same files we used in the [`list` method documentation](../file_system/list_method.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add four files to the pipeline we just created.\n",
    "output_1 = pipeline.process(\n",
    "    local_file_path=data_dir + \"input/frankenstein_very_short.txt\",  # the initial local filepath where the input JSON file is stored\n",
    "    local_save_directory=data_dir + \"output\",\n",
    "    expire_time=60 * 30,  # process data will be deleted from the Krixik system in 30 minutes\n",
    "    wait_for_process=True,  # do not wait for process to complete before returning IDE control to user\n",
    "    verbose=False,  # do not display process update printouts upon running code\n",
    "    symbolic_directory_path=\"/novels/gothic\",\n",
    "    file_name=\"Frankenstein.txt\",\n",
    ")\n",
    "\n",
    "output_2 = pipeline.process(\n",
    "    local_file_path=data_dir + \"input/pride_and_prejudice_very_short.txt\",  # the initial local filepath where the input JSON file is stored\n",
    "    local_save_directory=data_dir + \"output\",\n",
    "    expire_time=60 * 30,  # process data will be deleted from the Krixik system in 30 minutes\n",
    "    wait_for_process=True,  # do not wait for process to complete before returning IDE control to user\n",
    "    verbose=False,  # do not display process update printouts upon running code\n",
    "    symbolic_directory_path=\"/novels/romance\",\n",
    "    file_name=\"Pride and Prejudice.txt\",\n",
    ")\n",
    "\n",
    "output_3 = pipeline.process(\n",
    "    local_file_path=data_dir + \"input/moby_dick_very_short.txt\",  # the initial local filepath where the input JSON file is stored\n",
    "    local_save_directory=data_dir + \"output\",\n",
    "    expire_time=60 * 30,  # process data will be deleted from the Krixik system in 30 minutes\n",
    "    wait_for_process=True,  # do not wait for process to complete before returning IDE control to user\n",
    "    verbose=False,  # do not display process update printouts upon running code\n",
    "    symbolic_directory_path=\"/novels/adventure\",\n",
    "    file_name=\"Moby Dick.txt\",\n",
    ")\n",
    "\n",
    "output_4 = pipeline.process(\n",
    "    local_file_path=data_dir + \"input/little_women_very_short.txt\",  # the initial local filepath where the input JSON file is stored\n",
    "    local_save_directory=data_dir + \"output\",\n",
    "    expire_time=60 * 30,  # process data will be deleted from the Krixik system in 30 minutes\n",
    "    wait_for_process=True,  # do not wait for process to complete before returning IDE control to user\n",
    "    verbose=False,  # do not display process update printouts upon running code\n",
    "    symbolic_directory_path=\"/novels/bildungsroman\",\n",
    "    file_name=\"Little Women.txt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the output for one of these:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"status_code\": 200,\n",
      "  \"pipeline\": \"keyword_search_method_1_keyword-db\",\n",
      "  \"request_id\": \"4763d881-46e0-4d34-857b-bdd4cdf43aab\",\n",
      "  \"file_id\": \"ae071414-7192-4f78-a431-1a13c0f0bc4a\",\n",
      "  \"message\": \"SUCCESS - output fetched for file_id ae071414-7192-4f78-a431-1a13c0f0bc4a.Output saved to location(s) listed in process_output_files.\",\n",
      "  \"warnings\": [],\n",
      "  \"process_output\": null,\n",
      "  \"process_output_files\": [\n",
      "    \"../../../data/output/ae071414-7192-4f78-a431-1a13c0f0bc4a.db\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# nicely print the output of one of the above processes\n",
    "print(json.dumps(output_3, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value of `process_output` is `null` because the return object is an `SQLite` keyword database—one consisting of all non-trivial `(keyword, line_number, word_number)` tuples identified in the input file—so it cannot be printed here. You can review this database in the local location provided in `process_output_files`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Keyword Searches\n",
    "\n",
    "With files now processed through the pipeline we can run the `keyword_search` method on it.\n",
    "\n",
    "Let's try an example in which we search through one file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"status_code\": 200,\n",
      "  \"request_id\": \"5b252162-6ee5-4a48-ae62-f9954f4107f5\",\n",
      "  \"message\": \"\",\n",
      "  \"warnings\": [\n",
      "    {\n",
      "      \"WARNING: the following file_ids returned no results for the given query\": [\n",
      "        \"001dd7c5-87f6-4ffa-a647-060291d9679d\"\n",
      "      ]\n",
      "    }\n",
      "  ],\n",
      "  \"items\": []\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# perform keyword_search over one file\n",
    "keyword_output = pipeline.keyword_search(query=\"mansion adolescence party enemy romance\", file_names=[\"Little Women.txt\"])\n",
    "\n",
    "# nicely print the output of this search\n",
    "print(json.dumps(keyword_output, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `keyword_search` method returns each appearance of each searched-for keyword. As you can see, for every searched-through file there is an entry for every keyword appearance. The entry indicates both line number and the word number within that line.\n",
    "\n",
    "It works just as well when searching through several files by using the [wildcard operator](../file_system/list_method.md#wildcard-operator-arguments):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"status_code\": 200,\n",
      "  \"request_id\": \"5e53f349-ed65-4735-a288-d2effbb447a5\",\n",
      "  \"message\": \"Successfully queried the first 1 user file out of 4 defined by input query arguments.\",\n",
      "  \"warnings\": [\n",
      "    {\n",
      "      \"WARNING: the following file_ids returned no results for the given query\": [\n",
      "        \"ae071414-7192-4f78-a431-1a13c0f0bc4a\",\n",
      "        \"f59be5e9-2bd3-4a07-8a7e-1a52064c6ee1\",\n",
      "        \"001dd7c5-87f6-4ffa-a647-060291d9679d\"\n",
      "      ]\n",
      "    }\n",
      "  ],\n",
      "  \"items\": [\n",
      "    {\n",
      "      \"file_id\": \"786baed5-66ff-4bf4-941f-0baa562e9666\",\n",
      "      \"file_metadata\": {\n",
      "        \"file_name\": \"pride and prejudice.txt\",\n",
      "        \"symbolic_directory_path\": \"/novels/romance\",\n",
      "        \"file_tags\": [],\n",
      "        \"num_lines\": 40,\n",
      "        \"created_at\": \"2024-06-05 16:18:15\",\n",
      "        \"last_updated\": \"2024-06-05 16:18:15\"\n",
      "      },\n",
      "      \"search_results\": [\n",
      "        {\n",
      "          \"keyword\": \"party\",\n",
      "          \"line_number\": 23,\n",
      "          \"keyword_number\": 8\n",
      "        }\n",
      "      ]\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# perform keyword_search over multiple files\n",
    "keyword_output = pipeline.keyword_search(query=\"mansion adolescence party enemy romance\", symbolic_directory_paths=[\"/novels*\"])\n",
    "\n",
    "# nicely print the output of this search\n",
    "print(json.dumps(keyword_output, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Size Cap\n",
    "\n",
    "The current size limit on output generated by the `keyword_search` method is 5MB."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop Words\n",
    "\n",
    "'Stop words' are the words that are ignored by keyword search. There are words in the English language that are so common and so often used (e.g. \"the\", \"and\") that we assume that they will not be searched for. Consequently, the `keyword_search` method skips them if they are in the query, making for more focused results. There is at present no way to keyword search for any word in the stop words list, which you can review here:."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stop_words = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "with open(data_dir + \"other/stop_words.txt\", \"r\") as file:\n",
    "    print(file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# delete all processed datapoints belonging to this pipeline\n",
    "reset_pipeline(pipeline)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
