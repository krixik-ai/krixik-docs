{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/krixik-ai/krixik-docs/blob/main/docs/examples/single_module_pipelines/single_parser.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "remove_cell",
     "remove_output"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCESS: You are now authenticated.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import importlib\n",
    "from pathlib import Path\n",
    "\n",
    "# demo setup - including secrets instantiation, requirements installation, and path setting\n",
    "if os.getenv(\"COLAB_RELEASE_TAG\"):\n",
    "    # if running this notebook in collab - make sure to enter your secrets\n",
    "    MY_API_KEY = \"YOUR_API_KEY_HERE\"\n",
    "    MY_API_URL = \"YOUR_API_URL_HERE\"\n",
    "\n",
    "    # if running this notebook on collab - install requirements and pull required subdirectories\n",
    "    # install krixik python client\n",
    "    !pip install krixik\n",
    "\n",
    "    # install github clone - allows for easy cloning of subdirectories from docs repo: https://github.com/krixik-ai/krixik-docs\n",
    "    !pip install github-clone\n",
    "\n",
    "    # clone datasets\n",
    "    if not Path(\"data\").is_dir():\n",
    "        !ghclone https://github.com/krixik-ai/krixik-docs/tree/main/data\n",
    "    else:\n",
    "        print(\"docs datasets already cloned!\")\n",
    "\n",
    "    # define data dir\n",
    "    data_dir = \"./data/\"\n",
    "\n",
    "    # create output dir\n",
    "    from pathlib import Path\n",
    "\n",
    "    Path(data_dir + \"/output\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # pull utilities\n",
    "    if not Path(\"utilities\").is_dir():\n",
    "        !ghclone https://github.com/krixik-ai/krixik-docs/tree/main/utilities\n",
    "    else:\n",
    "        print(\"docs utilities already cloned!\")\n",
    "else:\n",
    "    # if running local pull of docs - set paths relative to local docs structure\n",
    "    # import utilities\n",
    "    sys.path.append(\"../../../\")\n",
    "\n",
    "    # define data_dir\n",
    "    data_dir = \"../../../data/\"\n",
    "\n",
    "    # if running this notebook locally from krixik docs repo - load secrets from a .env placed at the base of the docs repo\n",
    "    from dotenv import load_dotenv\n",
    "\n",
    "    load_dotenv(\"../../../.env\")\n",
    "\n",
    "    MY_API_KEY = os.getenv(\"MY_API_KEY\")\n",
    "    MY_API_URL = os.getenv(\"MY_API_URL\")\n",
    "\n",
    "\n",
    "# load in reset\n",
    "reset = importlib.import_module(\"utilities.reset\")\n",
    "reset_pipeline = reset.reset_pipeline\n",
    "\n",
    "\n",
    "# import krixik and initialize it with your personal secrets\n",
    "from krixik import krixik\n",
    "\n",
    "krixik.init(api_key=MY_API_KEY, api_url=MY_API_URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single-Module Pipeline: `parser`\n",
    "\n",
    "This document is a walkthrough of how to assemble and use a single-module pipeline that only includes a [`parser`](../../modules/support_function_modules/parser_module.md) module. It's divided into the following sections:\n",
    "\n",
    "- [Pipeline Setup](#pipeline-setup)\n",
    "- [Required Input Format](#required-input-format)\n",
    "- [Using the Default Model](#using-the-default-model)\n",
    "- [Using a Non-Default Model](#using-a-non-default-model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline Setup\n",
    "\n",
    "Let's first instantiate a single-module [`parser`](../../modules/support_function_modules/parser_module.md) pipeline.\n",
    "\n",
    "We use the [`.create_pipeline`](../../system/pipeline_creation/create_pipeline.md) method for this, passing only the [`parser`](../../modules/support_function_modules/parser_module.md) module name into `module_chain`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a pipeline with a single parser module\n",
    "pipeline = krixik.create_pipeline(name=\"single_parser_1\", module_chain=[\"parser\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Required Input Format\n",
    "\n",
    "The [`parser`](../../modules/support_function_modules/parser_module.md) module accepts document inputs. Acceptable file formats are TXT, PDF, DOCX, and PPTX, although the last three formats are automatically converted to TXT before processing.\n",
    "\n",
    "Let's take a quick look at a valid input file, and then process it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It was a bright cold day in April, and the clocks were striking thirteen.\n",
      "Winston Smith, his chin nuzzled into his breast in an effort to escape the\n",
      "vile wind, slipped quickly through the glass doors of Victory Mansions,\n",
      "though not quickly enough to prevent a swirl of gritty dust from entering\n",
      "along with him.\n"
     ]
    }
   ],
   "source": [
    "# examine contents of a valid test input file\n",
    "with open(data_dir + \"input/1984_very_short.txt\", \"r\") as file:\n",
    "    print(file.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the Default Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's process our test input file using the [`parser`](../../modules/support_function_modules/parser_module.md) module's [default model](../../modules/support_function_modules/parser_module.md#available-models-in-the-parser-module): [`sentence`](https://www.nltk.org/api/nltk.tokenize.html).\n",
    "\n",
    "Given that this is the default model, we need not specify model selection through the optional [`modules`](../../system/parameters_processing_files_through_pipelines/process_method.md#selecting-models-via-the-modules-argument) argument in the [`.process`](../../system/parameters_processing_files_through_pipelines/process_method.md) method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process the file with the default model\n",
    "process_output = pipeline.process(\n",
    "    local_file_path=data_dir + \"input/1984_very_short.txt\",  # the initial local filepath where the input file is stored\n",
    "    local_save_directory=data_dir + \"output\",  # the local directory that the output file will be saved to\n",
    "    expire_time=60 * 30,  # process data will be deleted from the Krixik system in 30 minutes\n",
    "    wait_for_process=True,  # wait for process to complete before returning IDE control to user\n",
    "    verbose=False,\n",
    ")  # do not display process update printouts upon running code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of this process is printed below. To learn more about each component of the output, review documentation for the [`.process`](../../system/parameters_processing_files_through_pipelines/process_method.md) method.\n",
    "\n",
    "Because the output of this particular module-model pair is a JSON file, the process output is provided in this object as well (this is only the case for JSON outputs).  Moreover, the output file itself has been saved to the location noted in the `process_output_files` key.  The `file_id` of the processed input is used as a filename prefix for the output file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"status_code\": 200,\n",
      "  \"pipeline\": \"single_parser_1\",\n",
      "  \"request_id\": \"07569a36-93d8-47bb-b487-bba25ccc1348\",\n",
      "  \"file_id\": \"60542629-7470-476f-b94e-40e2c53608bf\",\n",
      "  \"message\": \"SUCCESS - output fetched for file_id 60542629-7470-476f-b94e-40e2c53608bf.Output saved to location(s) listed in process_output_files.\",\n",
      "  \"warnings\": [],\n",
      "  \"process_output\": [\n",
      "    {\n",
      "      \"snippet\": \"It was a bright cold day in April, and the clocks were striking thirteen.\",\n",
      "      \"line_numbers\": [\n",
      "        1\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"snippet\": \"Winston Smith, his chin nuzzled into his breast in an effort to escape the\\nvile wind, slipped quickly through the glass doors of Victory Mansions,\\nthough not quickly enough to prevent a swirl of gritty dust from entering\\nalong with him.\",\n",
      "      \"line_numbers\": [\n",
      "        2,\n",
      "        3,\n",
      "        4,\n",
      "        5\n",
      "      ]\n",
      "    }\n",
      "  ],\n",
      "  \"process_output_files\": [\n",
      "    \"../../../data/output/60542629-7470-476f-b94e-40e2c53608bf.json\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# nicely print the output of this process\n",
    "print(json.dumps(process_output, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from `process_output` that our two-sentence paragraph input has been separated correctly. Each sentence is also accompanied by its corresponding line number(s).\n",
    "\n",
    "To confirm that everything went as it should have, let's load in the text file output from `process_output_files`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"snippet\": \"It was a bright cold day in April, and the clocks were striking thirteen.\",\n",
      "    \"line_numbers\": [\n",
      "      1\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"snippet\": \"Winston Smith, his chin nuzzled into his breast in an effort to escape the\\nvile wind, slipped quickly through the glass doors of Victory Mansions,\\nthough not quickly enough to prevent a swirl of gritty dust from entering\\nalong with him.\",\n",
      "    \"line_numbers\": [\n",
      "      2,\n",
      "      3,\n",
      "      4,\n",
      "      5\n",
      "    ]\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# load in process output from file\n",
    "with open(process_output[\"process_output_files\"][0]) as f:\n",
    "    print(json.dumps(json.load(f), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a Non-Default Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use a [non-default model](../../modules/support_function_modules/parser_module.md#available-models-in-the-parser-module) like `fixed`, we must enter it explicitly through the [`modules`](../../system/parameters_processing_files_through_pipelines/process_method.md#selecting-models-via-the-modules-argument) argument when invoking the [`.process`](../../system/parameters_processing_files_through_pipelines/process_method.md) method. Note that if you don't explicitly define parameters for the `fixed` model (for it is parameterizable) default values will be used instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process the file with a non-default model\n",
    "process_output = pipeline.process(\n",
    "    local_file_path=data_dir + \"input/1984_very_short.txt\",  # all parameters save 'modules' as above\n",
    "    local_save_directory=data_dir + \"output\",\n",
    "    expire_time=60 * 30,\n",
    "    wait_for_process=True,\n",
    "    verbose=False,\n",
    "    modules={\"parser\": {\"model\": \"fixed\", \"params\": {\"chunk_size\": 9, \"overlap_size\": 3}}},\n",
    ")  # specify a non-default model for this process as well as its parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can view the newly parsed text by loading in the output file, as below.\n",
    "\n",
    "Examining the output we can see that our input document was not cut into complete sentences, but instead into chunks of text as specified.  Each chunk is nine words in length, and the consecutive chunks overlap by three words. The parameterized model has worked as instructed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"snippet\": \"It was a bright cold day in April, and\",\n",
      "    \"line_numbers\": [\n",
      "      1\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"snippet\": \"in April, and the clocks were striking thirteen. Winston\",\n",
      "    \"line_numbers\": [\n",
      "      1,\n",
      "      2\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"snippet\": \"striking thirteen. Winston Smith, his chin nuzzled into his\",\n",
      "    \"line_numbers\": [\n",
      "      1,\n",
      "      2\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"snippet\": \"nuzzled into his breast in an effort to escape\",\n",
      "    \"line_numbers\": [\n",
      "      2\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"snippet\": \"effort to escape the vile wind, slipped quickly through\",\n",
      "    \"line_numbers\": [\n",
      "      2,\n",
      "      3\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"snippet\": \"slipped quickly through the glass doors of Victory Mansions,\",\n",
      "    \"line_numbers\": [\n",
      "      3\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"snippet\": \"of Victory Mansions, though not quickly enough to prevent\",\n",
      "    \"line_numbers\": [\n",
      "      3,\n",
      "      4\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"snippet\": \"enough to prevent a swirl of gritty dust from\",\n",
      "    \"line_numbers\": [\n",
      "      4\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"snippet\": \"gritty dust from entering along with him.\",\n",
      "    \"line_numbers\": [\n",
      "      4,\n",
      "      5\n",
      "    ]\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# load in process output from file\n",
    "with open(process_output[\"process_output_files\"][0]) as f:\n",
    "    print(json.dumps(json.load(f), indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# delete all processed datapoints belonging to this pipeline\n",
    "reset_pipeline(pipeline)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
