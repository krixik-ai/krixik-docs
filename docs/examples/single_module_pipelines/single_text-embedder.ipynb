{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/krixik-ai/krixik-docs/blob/main/docs/examples/single_module_pipelines/single_text-embedder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a> <a href=\"https://youtu.be/qNZIElHXmLA\" target=\"_parent\"><img src=\"https://badges.aleen42.com/src/youtube.svg\" alt=\"Youtube\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "remove_cell",
     "remove_output"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCESS: You are now authenticated.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# demo setup - including secrets instantiation, requirements installation, and path setting\n",
    "if os.getenv(\"COLAB_RELEASE_TAG\"):\n",
    "    # if running this notebook in Google Colab - make sure to enter your secrets\n",
    "    MY_API_KEY = \"YOUR_API_KEY_HERE\"\n",
    "    MY_API_URL = \"YOUR_API_URL_HERE\"\n",
    "\n",
    "    # if running this notebook on Google Colab - install requirements and pull required subdirectories\n",
    "    # install Krixik python client\n",
    "    !pip install krixik\n",
    "\n",
    "    # install github clone - allows for easy cloning of subdirectories from docs repo: https://github.com/krixik-ai/krixik-docs\n",
    "    !pip install github-clone\n",
    "\n",
    "    # clone datasets\n",
    "    if not Path(\"data\").is_dir():\n",
    "        !ghclone https://github.com/krixik-ai/krixik-docs/tree/main/data\n",
    "    else:\n",
    "        print(\"docs datasets already cloned!\")\n",
    "\n",
    "    # define data dir\n",
    "    data_dir = \"./data/\"\n",
    "\n",
    "    # create output dir\n",
    "    from pathlib import Path\n",
    "\n",
    "    Path(data_dir + \"/output\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "else:\n",
    "    # if running local pull of docs - set paths relative to local docs structure\n",
    "    # import utilities\n",
    "    sys.path.append(\"../../../\")\n",
    "\n",
    "    # define data_dir\n",
    "    data_dir = \"../../../data/\"\n",
    "\n",
    "    # if running this notebook locally from Krixik docs repo - load secrets from a .env placed at the base of the docs repo\n",
    "    from dotenv import load_dotenv\n",
    "\n",
    "    load_dotenv(\"../../../.env\")\n",
    "\n",
    "    MY_API_KEY = os.getenv(\"MY_API_KEY\")\n",
    "    MY_API_URL = os.getenv(\"MY_API_URL\")\n",
    "\n",
    "# import Krixik and initialize it with your personal secrets\n",
    "from krixik import krixik\n",
    "\n",
    "krixik.init(api_key=MY_API_KEY, api_url=MY_API_URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single-Module Pipeline: `text-embedder`\n",
    "[ðŸ‡¨ðŸ‡´ VersiÃ³n en espaÃ±ol de este documento](https://krixik-docs.readthedocs.io/es-main/ejemplos/ejemplos_pipelines_modulo_unico/unico_text-embedder_encaje_lexico/)\n",
    "\n",
    "This document is a walkthrough of how to assemble and use a single-module pipeline that only includes a [`text-embedder`](../../modules/ai_modules/text-embedder_module.md) module. \n",
    "\n",
    "Text embedding involves converting words or sentences into numerical vectors in a high-dimensional space. The vectors are mathematical representations that preserve the semantic relationships and contextual meanings of the text. Its applications lie in improving Natural Language Processing (NLP) tasks like semantic/vector search, sentiment analysis, machine translation, information retrieval, and recommendation systems, among other possibilities.\n",
    "\n",
    "For a slightly more complex pipeline that combines a [`text-embedder`](../../modules/ai_modules/text-embedder_module.md) module with a [`vector-db`](../../modules/database_modules/vector-db_module.md) module to enable [`semantic (vector) search`](../../system/search_methods/semantic_search_method.md), [click here](../search_pipeline_examples/multi_basic_semantic_search.md).\n",
    "\n",
    "The document is divided into the following sections:\n",
    "\n",
    "- [Pipeline Setup](#pipeline-setup)\n",
    "- [Required Input Format](#required-input-format)\n",
    "- [Using the Default Model](#using-the-default-model)\n",
    "- [Examining Process Output Locally](#examining-process-output-locally)\n",
    "- [Using a Non-Default Model](#using-a-non-default-model)\n",
    "- [Using the `semantic_search` Method](#using-the-semantic_search-method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline Setup\n",
    "\n",
    "Let's first instantiate a single-module [`text-embedder`](../../modules/ai_modules/text-embedder_module.md) pipeline.\n",
    "\n",
    "We use the [`create_pipeline`](../../system/pipeline_creation/create_pipeline.md) method for this, passing only the [`text-embedder`](../../modules/ai_modules/text-embedder_module.md) module name into `module_chain`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a pipeline with a single text-embedder module\n",
    "pipeline = krixik.create_pipeline(name=\"single_text-embedder-1\", module_chain=[\"text-embedder\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Required Input Format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [`text-embedder`](../../modules/ai_modules/text-embedder_module.md) module accepts JSON file input. The input JSON must respect [this format](../../system/parameters_processing_files_through_pipelines/JSON_input_format.md).\n",
    "\n",
    "The JSON file may optionally also include, along with each snippet, a key-value pair in which the key is the string `\"line numbers\"` and the value is an integer list of every line number of the original document that the snippet is on. This will make it easier for you to identify what document line each vector is an embedding of.\n",
    "\n",
    "Let's take a quick look at a valid input file, and then process it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"snippet\": \"It was a bright cold day in April, and the clocks were striking thirteen.\",\n",
      "    \"line_numbers\": [\n",
      "      1\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"snippet\": \"Winston Smith, his chin nuzzled into his breast in an effort to escape the\\nvile wind, slipped quickly through the glass doors of Victory Mansions,\\nthough not quickly enough to prevent a swirl of gritty dust from entering\\nalong with him.\",\n",
      "    \"line_numbers\": [\n",
      "      2,\n",
      "      3,\n",
      "      4,\n",
      "      5\n",
      "    ]\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# examine contents of a valid input file\n",
    "with open(data_dir + \"input/1984_snippets.json\", \"r\") as file:\n",
    "    print(json.dumps(json.load(file), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the Default Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's process our test input file using the [`text-embedder`](../../modules/ai_modules/text-embedder_module.md) module's [default model](../../modules/ai_modules/text-embedder_module.md#available-models-in-the-text-embedder-module): [`all-MiniLM-L6-v2`](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2).\n",
    "\n",
    "Given that this is the default model, we need not specify model selection through the optional [`modules`](../../system/parameters_processing_files_through_pipelines/process_method.md#selecting-models-via-the-modules-argument) argument in the [`process`](../../system/parameters_processing_files_through_pipelines/process_method.md) method.\n",
    "\n",
    "In a later section of this document we will process the same file again, but select our model and the quantization thereof explicitly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process the file with the default model\n",
    "process_output = pipeline.process(\n",
    "    local_file_path=data_dir + \"input/1984_snippets.json\",  # the initial local filepath where the input file is stored\n",
    "    local_save_directory=data_dir + \"output\",  # the local directory that the output file will be saved to\n",
    "    expire_time=60 * 30,  # process data will be deleted from the Krixik system in 30 minutes\n",
    "    wait_for_process=True,  # wait for process to complete before returning IDE control to user\n",
    "    verbose=False,\n",
    ")  # do not display process update printouts upon running code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of this process is printed below. To learn more about each component of the output, review documentation for the [`process`](../../system/parameters_processing_files_through_pipelines/process_method.md) method.\n",
    "\n",
    "Moreover, the output file itself has been saved to the location noted in the `process_output_files` key.  The `file_id` of the processed input is used as a filename prefix for the output file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"status_code\": 200,\n",
      "  \"pipeline\": \"single_text-embedder-1\",\n",
      "  \"request_id\": \"ce2e57ce-c2be-49ac-8d43-59e6db2bcf25\",\n",
      "  \"file_id\": \"ce4ddfa5-12c6-4dcb-86af-4f6d30ed6188\",\n",
      "  \"message\": \"SUCCESS - output fetched for file_id ce4ddfa5-12c6-4dcb-86af-4f6d30ed6188.Output saved to location(s) listed in process_output_files.\",\n",
      "  \"warnings\": [],\n",
      "  \"process_output\": null,\n",
      "  \"process_output_files\": [\n",
      "    \"../../../data/output/ce4ddfa5-12c6-4dcb-86af-4f6d30ed6188.npy\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# nicely print the output of this process\n",
    "print(json.dumps(process_output, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examining Process Output Locally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The outputted NPY file containing embedding vectors of our input data can be examined as follows. For the sake of clarity, in this example we will simply print the shape, and not any of the contents, of the returned array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 384)\n"
     ]
    }
   ],
   "source": [
    "# examine vector output\n",
    "import numpy as np\n",
    "\n",
    "vectors = np.load(process_output[\"process_output_files\"][0])\n",
    "print(vectors.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In other words, the array has 2 rows with 384 values in each row. \n",
    "\n",
    "In the context of the input file, the first row is the vectorized form of our first snippet: \"It was a bright cold day in April, and the clocks were striking thirteen.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a Non-Default Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use a [non-default model](../../modules/ai_modules/text-embedder_module.md#available-models-in-the-text-embedder-module) like [`all-mpnet-base-v2`](https://huggingface.co/sentence-transformers/all-mpnet-base-v2), we must enter it explicitly through the [`modules`](../../system/parameters_processing_files_through_pipelines/process_method.md#selecting-models-via-the-modules-argument) argument when invoking the [`process`](../../system/parameters_processing_files_through_pipelines/process_method.md) method. As [module documentation](../../modules/ai_modules/text-embedder_module.md) indicates, you can also specify whether or not you wish to use the quantized version of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process the file with a non-default model\n",
    "process_output = pipeline.process(\n",
    "    local_file_path=data_dir + \"input/1984_snippets.json\",  # all parameters save 'modules' as above\n",
    "    local_save_directory=data_dir + \"output\",\n",
    "    expire_time=60 * 30,\n",
    "    wait_for_process=True,\n",
    "    verbose=False,\n",
    "    modules={\"text-embedder\": {\"model\": \"all-mpnet-base-v2\", \"params\": {\"quantize\": False}}},\n",
    ")  # specify a non-default model for this process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can examine the output as we did above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"status_code\": 200,\n",
      "  \"pipeline\": \"single_text-embedder-1\",\n",
      "  \"request_id\": \"f53c93da-cf1b-41e0-a578-16dc62736ed4\",\n",
      "  \"file_id\": \"1dcbde04-d4f2-414f-acaf-577c355bbb88\",\n",
      "  \"message\": \"SUCCESS - output fetched for file_id 1dcbde04-d4f2-414f-acaf-577c355bbb88.Output saved to location(s) listed in process_output_files.\",\n",
      "  \"warnings\": [],\n",
      "  \"process_output\": null,\n",
      "  \"process_output_files\": [\n",
      "    \"../../../data/output/1dcbde04-d4f2-414f-acaf-577c355bbb88.npy\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# nicely print the output of this process\n",
    "print(json.dumps(process_output, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the `semantic_search` method\n",
    "\n",
    "Any pipeline containing a [`vector-db`](../../modules/database_modules/vector-db_module.md) module preceded by a [`text-embedder`](../../modules/ai_modules/text-embedder_module.md) module has access to the [`semantic_search`](../../system/search_methods/semantic_search_method.md) method. This provides you with the convenient ability to effect semantic queries on the created vector database(s).\n",
    "\n",
    "As the single-module pipeline created above lacks the [`vector-db`](../../modules/database_modules/vector-db_module.md) module, the [`semantic_search`](../../system/search_methods/semantic_search_method.md) method will not work on it. Review documentation for this [pipeline example](../../examples/search_pipeline_examples/multi_basic_semantic_search.md) or this [pipeline example](../../examples/search_pipeline_examples/multi_snippet_semantic_search.md), both of which meet the requirements for the method: the former ingests TXT files, and the latter JSON files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# delete all processed datapoints belonging to this pipeline\n",
    "krixik.reset_pipeline(pipeline)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
