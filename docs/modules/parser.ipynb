{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The parser module\n",
    "\n",
    "This document reviews the `parser` module - which takes in input documents, cuts them up into pieces using different model logic, and returns the spliced input as json output. \n",
    "\n",
    "This document includes an overview of custom pipeline setup, current model set, parameters, and `.process` usage for this module.\n",
    "\n",
    "To follow along with this demonstration be sure to initialize your krixik session with your api key and url as shown below. \n",
    "\n",
    "We illustrate loading these required secrets in via [python-dotenv](https://pypi.org/project/python-dotenv/), storing those secrets in a `.env` file.  This is always good practice for storing / loading secrets (e.g., doing so will reduce the chance you inadvertantly push secrets to a repo)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append('../../')\n",
    "import importlib\n",
    "reset = importlib.import_module(\"utilities.reset\")\n",
    "reset_pipeline = reset.reset_pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "remove_output"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCESS: You are now authenticated.\n"
     ]
    }
   ],
   "source": [
    "# load secrets from a .env file using python-dotenv\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(\"../.env\")\n",
    "MY_API_KEY = os.getenv(\"MY_API_KEY\")\n",
    "MY_API_URL = os.getenv(\"MY_API_URL\")\n",
    "\n",
    "# import krixik and initialize it with your personal secrets\n",
    "from krixik import krixik\n",
    "\n",
    "krixik.init(api_key=MY_API_KEY, api_url=MY_API_URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This small function prints dictionaries very nicely in notebooks / markdown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print dictionaries / json nicely in notebooks / markdown\n",
    "import json\n",
    "def json_print(data):\n",
    "    print(json.dumps(data, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A table of contents for the remainder of this document is shown below.\n",
    "\n",
    "\n",
    "- [pipeline setup](#pipeline-setup)\n",
    "- [required input format](#required-input-format)\n",
    "- [using the default model](#using-the-default-model)\n",
    "- [using a non-default model](#using-a-non-default-model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline setup\n",
    "\n",
    "Below we setup a simple one module pipeline using the `parser` module.  This parser takes in an input text file and splits into its constituent snippets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a pipeline with a single module\n",
    "pipeline = krixik.create_pipeline(name=\"my-parser-pipeline\", module_chain=[\"parser\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `parser` module comes with two models that determine how it cuts up an input text:\n",
    "\n",
    "- `sentence`: (default) splits a text into its individual sentences\n",
    "- `fixed`: splits a text into potentially overlapping chunks of consecutive words\n",
    "\n",
    "The `fixed` model takes in two parameters to determine how it operates:\n",
    "\n",
    "- `chunk_size` (recommended default 10) chunk size length in number of consecutive words\n",
    "- `overlap_size`: (recommended default 2) length of overlap in words between consecutive chunks\n",
    "\n",
    "These available modeling options and parameters are stored in our custom pipeline's configuration (described further in LINK HERE).  We can examine this configuration as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"pipeline\": {\n",
      "    \"name\": \"my-parser-pipeline\",\n",
      "    \"modules\": [\n",
      "      {\n",
      "        \"name\": \"parser\",\n",
      "        \"models\": [\n",
      "          {\n",
      "            \"name\": \"sentence\"\n",
      "          },\n",
      "          {\n",
      "            \"name\": \"fixed\",\n",
      "            \"params\": {\n",
      "              \"chunk_size\": {\n",
      "                \"type\": \"int\",\n",
      "                \"default\": 10\n",
      "              },\n",
      "              \"overlap_size\": {\n",
      "                \"type\": \"int\",\n",
      "                \"default\": 4\n",
      "              }\n",
      "            }\n",
      "          }\n",
      "        ],\n",
      "        \"defaults\": {\n",
      "          \"model\": \"sentence\"\n",
      "        },\n",
      "        \"input\": {\n",
      "          \"type\": \"text\",\n",
      "          \"permitted_extensions\": [\n",
      "            \".txt\",\n",
      "            \".pdf\",\n",
      "            \".docx\",\n",
      "            \".pptx\"\n",
      "          ]\n",
      "        },\n",
      "        \"output\": {\n",
      "          \"type\": \"json\"\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# nicely print pipeline configuration\n",
    "json_print(pipeline.config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see the models and their associated parameters available for use.\n",
    "\n",
    "You can save this configuration to disk as well by executing\n",
    "\n",
    "\n",
    "```python\n",
    "pipeline.save_pipeline(\"/valid/path/file.yml\")\n",
    "```\n",
    "\n",
    "You can instantiate a pipeline directly from its configuration using the [.load_pipeline method](LINK HERE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# delete all processed datapoints belonging to this pipeline\n",
    "reset_pipeline(pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required input format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `keyword-db` module accepts `.txt`, `.pdf`, `.docx`, and `.pptx` file formats as input.  The latter three (`.pdf`, `.docx`, and `.pptx`) are first converted to `.txt` prior to processing.\n",
    "\n",
    "Let's look at an example of a small valid input - and then process it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It was a bright cold day in April, and the clocks were striking thirteen.\n",
      "Winston Smith, his chin nuzzled into his breast in an effort to escape the\n",
      "vile wind, slipped quickly through the glass doors of Victory Mansions,\n",
      "though not quickly enough to prevent a swirl of gritty dust from entering\n",
      "along with him.\n"
     ]
    }
   ],
   "source": [
    "# examine contents of a valid test input file\n",
    "test_file = \"../../data/input/1984_very_short.txt\"\n",
    "with open(test_file, \"r\") as file:\n",
    "    print(file.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the default model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's process the input file above using the default model - `sentence`.  Because `sentence` is the default model we need not input the optional `modules` argument into `.process`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define path to an input file from examples directory\n",
    "test_file = \"../../data/input/1984_very_short.txt\"\n",
    "\n",
    "# process for search\n",
    "process_output = pipeline.process(local_file_path=test_file,\n",
    "                                  local_save_directory=\"../../data/output\", # save output repo data output subdir\n",
    "                                  expire_time=60 * 10,    # set all process data to expire in 10 minutes\n",
    "                                  wait_for_process=True,    # wait for process to complete before regaining ide\n",
    "                                  verbose=False)            # set verbosity to False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of this process is printed below.  Because the output of this particular module-model pair is json, the process output is provided in the return response.  The output file itself has been returned to the address noted in the `process_output_files` key.  The `file_id` of the processed input is used as a filename prefix for the output file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"status_code\": 200,\n",
      "  \"pipeline\": \"my-parser-pipeline\",\n",
      "  \"request_id\": \"b394e1c4-0032-439a-a2f8-8fa58f0140fe\",\n",
      "  \"file_id\": \"82634bd4-476a-4e46-95cf-7e532046354e\",\n",
      "  \"message\": \"SUCCESS - output fetched for file_id 82634bd4-476a-4e46-95cf-7e532046354e.Output saved to location(s) listed in process_output_files.\",\n",
      "  \"warnings\": [],\n",
      "  \"process_output\": [\n",
      "    {\n",
      "      \"snippet\": \"It was a bright cold day in April, and the clocks were striking thirteen.\",\n",
      "      \"line_numbers\": [\n",
      "        1\n",
      "      ]\n",
      "    },\n",
      "    {\n",
      "      \"snippet\": \"Winston Smith, his chin nuzzled into his breast in an effort to escape the\\nvile wind, slipped quickly through the glass doors of Victory Mansions,\\nthough not quickly enough to prevent a swirl of gritty dust from entering\\nalong with him.\",\n",
      "      \"line_numbers\": [\n",
      "        2,\n",
      "        3,\n",
      "        4,\n",
      "        5\n",
      "      ]\n",
      "    }\n",
      "  ],\n",
      "  \"process_output_files\": [\n",
      "    \"../../data/output/82634bd4-476a-4e46-95cf-7e532046354e.json\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# nicely print the output of this process\n",
    "json_print(process_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets break down the output:\n",
    "\n",
    "- `status_code`: provides the success / failure signal for the api\n",
    "- `pipeline`: the name of the pipeline we ran `.process` on\n",
    "- `request_id`: unique id associated with this execution of `.process`\n",
    "- `file_id`: unique id for the processed file and its associated output\n",
    "- `message`: message detailing success or failure of call\n",
    "- `warnings`: message list indicating any warnings related to our call\n",
    "- `process_output`: returned output (available when module-model output is json only)\n",
    "- `process_output_files`: list of process output, local file names \n",
    "\n",
    "We can see from `process_output` that our two-sentence paragraph input has been separated correctly.  Each sentence also has its corresponding line number(s).\n",
    "\n",
    "This process output is also stored in the file contained in `process_output_files`.  Lets load it in and confirm we have the same process output we see above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"snippet\": \"It was a bright cold day in April, and the clocks were striking thirteen.\",\n",
      "    \"line_numbers\": [\n",
      "      1\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"snippet\": \"Winston Smith, his chin nuzzled into his breast in an effort to escape the\\nvile wind, slipped quickly through the glass doors of Victory Mansions,\\nthough not quickly enough to prevent a swirl of gritty dust from entering\\nalong with him.\",\n",
      "    \"line_numbers\": [\n",
      "      2,\n",
      "      3,\n",
      "      4,\n",
      "      5\n",
      "    ]\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# load in process output from file\n",
    "import json\n",
    "\n",
    "with open(process_output[\"process_output_files\"][0], \"r\") as file:\n",
    "    json_print(json.load(file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a non-default model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use a non-default model like `fixed` we pass its name explicitly via the `modules` argument as follows.  This will implicitly pass the default parameter values for the `fixed` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define path to an input file from examples directory\n",
    "test_file = \"../../data/input/1984_very_short.txt\"\n",
    "\n",
    "# process for search\n",
    "process_output = pipeline.process(local_file_path=test_file,\n",
    "                                  local_save_directory=\"../../data/output\", # save output repo data output subdir\n",
    "                                  expire_time=60 * 10,    # set all process data to expire in 10 minutes\n",
    "                                  wait_for_process=True,  # wait for process to complete before regaining ide\n",
    "                                  verbose=False,          # set verbosity to False\n",
    "                                  modules={\n",
    "                                    \"parser\": {\"model\": \"fixed\", \"params\": {\"chunk_size\": 10, \"overlap_size\": 2}}\n",
    "                                  })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examining the output below we can see that our input document was not cut into complete sentences, but chunks of text.  Each chunk is 10 words in length, and the consecutive chunks overlap by two words.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"snippet\": \"It was a bright cold day in April, and the\",\n",
      "    \"line_numbers\": [\n",
      "      1\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"snippet\": \"and the clocks were striking thirteen. Winston Smith, his chin\",\n",
      "    \"line_numbers\": [\n",
      "      1,\n",
      "      2\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"snippet\": \"his chin nuzzled into his breast in an effort to\",\n",
      "    \"line_numbers\": [\n",
      "      2\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"snippet\": \"effort to escape the vile wind, slipped quickly through the\",\n",
      "    \"line_numbers\": [\n",
      "      2,\n",
      "      3\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"snippet\": \"through the glass doors of Victory Mansions, though not quickly\",\n",
      "    \"line_numbers\": [\n",
      "      3,\n",
      "      4\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"snippet\": \"not quickly enough to prevent a swirl of gritty dust\",\n",
      "    \"line_numbers\": [\n",
      "      4\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"snippet\": \"gritty dust from entering along with him.\",\n",
      "    \"line_numbers\": [\n",
      "      4,\n",
      "      5\n",
      "    ]\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# load in process output from file\n",
    "import json\n",
    "\n",
    "with open(process_output[\"process_output_files\"][0], \"r\") as file:\n",
    "    json_print(json.load(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# delete all processed datapoints belonging to this pipeline\n",
    "reset_pipeline(pipeline)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
