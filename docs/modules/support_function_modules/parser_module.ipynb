{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The `parser` Module\n",
    "\n",
    "The `parser` module takes a text document, cuts it up into pieces, and returns the fragmented input as snippets in a JSON file.\n",
    "\n",
    "This overview of the `parser` module is divided into the following sections:\n",
    "\n",
    "- [Inputs and Outputs of the `parser` Module](#inputs-and-outputs-of-the-parser-module)\n",
    "- [Available Models in the `parser` Module](#available-models-in-the-parser-module)\n",
    "- [Model Parameters in the `parser` Module](#model-parameters-in-the-parser-module)\n",
    "- [Input File Size Limit](#input-file-size-limit)\n",
    "- [A Single-Module Pipeline for the `parser` Module](#a-single-module-pipeline-for-the-parser-module)\n",
    "- [Further Information on `parser` Module IO and Clickability](#further-information-on-parser-module-io-and-clickability)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inputs and Outputs of the `parser` Module\n",
    "\n",
    "The `parser` module accepts textual document inputs. Acceptable file formats are the following:\n",
    "\n",
    "- TXT\n",
    "\n",
    "- PDF (automatically converted to TXT before processing)\n",
    "\n",
    "- DOCX (automatically converted to TXT before processing)\n",
    "\n",
    "- PPTX (automatically converted to TXT before processing)\n",
    "\n",
    "The `parser` module returns a JSON file that contains all of the post-parsing text snippets. Each snippet is accompanied by its corresponding line numbers (from the original document) to make it easier for you to later know where in the document any single snippet came from. For example, take a look at the following sample output of a `parser` process:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "{\n",
    "  \"status_code\": 200,\n",
    "  \"pipeline\": \"modules-parser-docs\",\n",
    "  \"request_id\": \"5908efbc-b06d-44f3-93c8-a46c29540637\",\n",
    "  \"file_id\": \"575c69c6-0571-4f56-8e49-6c1e4f4a3f4a\",\n",
    "  \"message\": \"SUCCESS - output fetched for file_id 575c69c6-0571-4f56-8e49-6c1e4f4a3f4a.Output saved to location(s) listed in process_output_files.\",\n",
    "  \"warnings\": [],\n",
    "  \"process_output\": [\n",
    "    {\n",
    "      \"snippet\": \"It was a bright cold day in April, and the clocks were striking thirteen.\",\n",
    "      \"line_numbers\": [\n",
    "        1\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"snippet\": \"Winston Smith, his chin nuzzled into his breast in an effort to escape the\\nvile wind, slipped quickly through the glass doors of Victory Mansions,\\nthough not quickly enough to prevent a swirl of gritty dust from entering\\nalong with him.\",\n",
    "      \"line_numbers\": [\n",
    "        2,\n",
    "        3,\n",
    "        4,\n",
    "        5\n",
    "      ]\n",
    "    }\n",
    "  ],\n",
    "  \"process_output_files\": [\n",
    "    \"../../data/output/575c69c6-0571-4f56-8e49-6c1e4f4a3f4a.json\"\n",
    "  ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Available Models in the `parser` Module\n",
    "\n",
    "You can activate any of the following models when using the `parser` module:\n",
    "\n",
    "- [sentence](https://www.nltk.org/api/nltk.tokenize.html) - (default)\n",
    "\n",
    "- `fixed` - Krixik-made. Splits a text into potentially overlapping chunks of consecutive words that always have the same length.\n",
    "\n",
    "Use the [`modules`](../../system/parameters_processing_files_through_pipelines/process_method.md#selecting-models-via-the-modules-argument) argument in the [`process`](../../system/parameters_processing_files_through_pipelines/process_method.md) method to determine what model you'd like active when you process files through the `parser` module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Parameters in the `parser` Module\n",
    "\n",
    "Different parameter sets apply for the different `parser` models.\n",
    "\n",
    "The [`sentence`](https://www.nltk.org/api/nltk.tokenize.html) (default) model _is not_ parameterizable. Consequently, if selecting that model through the [`process`](../../system/parameters_processing_files_through_pipelines/process_method.md) method's [`modules`](../../system/parameters_processing_files_through_pipelines/process_method.md#selecting-models-via-the-modules-argument) argument, `params` will be set to an empty dictionary, as follows:\n",
    "\n",
    "```python\n",
    "# example model selection for parser module in process\n",
    "modules={'parser': {'model':'sentence',\n",
    "                    'params': {}}}\n",
    "```\n",
    "\n",
    "The `fixed` model _is_ parameterizable. Consequently, if selecting that model through the [`process`](../../system/parameters_processing_files_through_pipelines/process_method.md) method's [`modules`](../../system/parameters_processing_files_through_pipelines/process_method.md#selecting-models-via-the-modules-argument) argument, `params` can include a value for two different parameters:\n",
    "\n",
    "- `chunk_size` (int) - Number of consecutive words (a.k.a. tokens) in every chunk/snippet. Defaults to 10.\n",
    "- `overlap_size` (int) - Number of words that a chunk/snippet overlaps/shares with the previous chunk. If 0, chunks are lined up end-to-end. Defaults to 2.\n",
    "\n",
    " For example:\n",
    "\n",
    "```python\n",
    "# example model selection for parser module in process\n",
    "modules={'parser': {\"model\": \"fixed\",\n",
    "                    \"params\": {\"chunk_size\": 8, \"overlap_size\": 3}}}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input File Size Limit\n",
    "\n",
    "`parser` module input TXT files can currently be no larger than 2MB.\n",
    "\n",
    "DOCX, PDF, and PPTX input files can currently be no larger than 100MB. Once they are converted to TXT, the resultant TXT file will then be held to the aforementioned 2MB limit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Single-Module Pipeline for the `parser` Module\n",
    "\n",
    "Please click [here](../../examples/single_module_pipelines/single_parser.md) to visit the `Pipeline Examples` section of our documentation and review an example of a single-module pipeline for the `parser` module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further Information on `parser` Module IO and Clickability\n",
    "\n",
    "Please click [here](../../system/convenience_methods/convenience_methods.md) to visit the `Convenience Methods (and More!)` documentation. There you will find two tools to learn more about the `parser` module:\n",
    "\n",
    "- [View Module Input and Output Examples](../../system/convenience_methods/convenience_methods.md#view-module-input-and-output-examples)\n",
    "\n",
    "- [View Module Click Data with the `click_data` Method](../../system/convenience_methods/convenience_methods.md#view-module-click-data-with-the-click_data-method)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
