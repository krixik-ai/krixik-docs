{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The `vector-db` module\n",
    "\n",
    "This document reviews the `vector-db` module - which takes as input a numpy array, indexes its vectors, and returns an indexed [faiss database](https://github.com/facebookresearch/faiss).\n",
    "\n",
    "This document includes an overview of custom pipeline setup, current model set, parameters, and `.process` usage for this module.\n",
    "\n",
    "To follow along with this demonstration be sure to initialize your krixik session with your api key and url as shown below. \n",
    "\n",
    "We illustrate loading these required secrets in via [python-dotenv](https://pypi.org/project/python-dotenv/), storing those secrets in a `.env` file.  This is always good practice for storing / loading secrets (e.g., doing so will reduce the chance you inadvertantly push secrets to a repo)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "remove_convert"
    ]
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "from docs.utilities.reset import reset_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCESS: You are now authenticated.\n"
     ]
    }
   ],
   "source": [
    "# load secrets from a .env file using python-dotenv\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(\"../.env\")\n",
    "MY_API_KEY = os.getenv(\"MY_API_KEY\")\n",
    "MY_API_URL = os.getenv(\"MY_API_URL\")\n",
    "\n",
    "# import krixik and initialize it with your personal secrets\n",
    "from krixik import krixik\n",
    "\n",
    "krixik.init(api_key=MY_API_KEY, api_url=MY_API_URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This small function prints dictionaries very nicely in notebooks / markdown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print dictionaries / json nicely in notebooks / markdown\n",
    "import json\n",
    "\n",
    "\n",
    "def json_print(data):\n",
    "    print(json.dumps(data, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A table of contents for the remainder of this document is shown below.\n",
    "\n",
    "\n",
    "- [pipeline setup](#pipeline-setup)\n",
    "- [required input format](#required-input-format)\n",
    "- [using the default model](#using-the-default-model)\n",
    "- [using the `keyword_search` method](#using-the-keyword-search-method)\n",
    "- [querying output databases locally](#querying-output-databases-locally)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline setup\n",
    "\n",
    "Below we setup a simple one module pipeline using the `keyword-search` module. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "module_chain item -vector-d - is not a currently one of the currently available modules -['caption', 'json-to-txt', 'keyword-db', 'ocr', 'parser', 'sentiment', 'summarize', 'text-embedder', 'transcribe', 'translate', 'vector-db']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m \u001b[43mkrixik\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvector-db-pipeline-1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mmodule_chain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvector-d\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/krixik-cli/docs/modules/../../krixik/main.py:70\u001b[0m, in \u001b[0;36mkrixik.create_pipeline\u001b[0;34m(cls, name, module_chain)\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodule_chain must be a list of strings - the following item in it is not a string - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m item \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m available_modules:\n\u001b[0;32m---> 70\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodule_chain item -\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - is not a currently one of the currently available modules -\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavailable_modules\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     71\u001b[0m module_chain_ \u001b[38;5;241m=\u001b[39m [Module(m_name) \u001b[38;5;28;01mfor\u001b[39;00m m_name \u001b[38;5;129;01min\u001b[39;00m module_chain]\n\u001b[1;32m     72\u001b[0m custom \u001b[38;5;241m=\u001b[39m CreatePipeline(name\u001b[38;5;241m=\u001b[39mname,\n\u001b[1;32m     73\u001b[0m                         module_chain\u001b[38;5;241m=\u001b[39mmodule_chain_)\n",
      "\u001b[0;31mValueError\u001b[0m: module_chain item -vector-d - is not a currently one of the currently available modules -['caption', 'json-to-txt', 'keyword-db', 'ocr', 'parser', 'sentiment', 'summarize', 'text-embedder', 'transcribe', 'translate', 'vector-db']"
     ]
    }
   ],
   "source": [
    "# create a pipeline with a single module\n",
    "pipeline = krixik.create_pipeline(name=\"my-vector-db-pipeline\",\n",
    "                                  module_chain=[\"vector-db\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `vector-search` module comes with a single model:\n",
    "\n",
    "- `faiss`: (default) indexes a numpy array of input vectors\n",
    "\n",
    "These available modeling options and parameters are stored in our custom pipeline's configuration (described further in LINK HERE).  We can examine this configuration as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"pipeline\": {\n",
      "    \"name\": \"vector-db-pipeline-1\",\n",
      "    \"modules\": [\n",
      "      {\n",
      "        \"name\": \"vector-db\",\n",
      "        \"models\": [\n",
      "          {\n",
      "            \"name\": \"faiss\"\n",
      "          }\n",
      "        ],\n",
      "        \"defaults\": {\n",
      "          \"model\": \"faiss\"\n",
      "        },\n",
      "        \"input\": {\n",
      "          \"type\": \"npy\",\n",
      "          \"permitted_extensions\": [\n",
      "            \".npy\"\n",
      "          ]\n",
      "        },\n",
      "        \"output\": {\n",
      "          \"type\": \"faiss\"\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# nicely print the configuration of uor custom pipeline\n",
    "json_print(custom.config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see the models and their associated parameters available for use.\n",
    "\n",
    "You can save this configuration to disk as well by executing\n",
    "\n",
    "\n",
    "```python\n",
    "pipeline.save_pipeline(\"/valid/path/file.yml\")\n",
    "```\n",
    "\n",
    "You can instantiate a pipeline directly from its configuration using the [.load_pipeline method](LINK HERE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": [
     "remove_convert"
    ]
   },
   "outputs": [],
   "source": [
    "# delete all processed datapoints belonging to this pipeline\n",
    "reset_pipeline(pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required input format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `vector-db` module accepts `.npy` consisting of a single numpy array.  Each row is a vector to be indexed for vector search.\n",
    "\n",
    "Let's look at an example of a small valid input - and then process it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# examine contents of input file\n",
    "import numpy as np\n",
    "test_file = \"../input_data/vectors.npy\"\n",
    "np.load(test_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the default model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's process it using the default model - `faiss`.  Because `faiss` is the default model we need not input the optional `modules` argument into `.process`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define path to an input file from examples directory\n",
    "test_file = \"../input_data/vectors.npy\"\n",
    "\n",
    "# process for search\n",
    "process_output = pipeline.process(\n",
    "    local_file_path=test_file,\n",
    "    local_save_directory=\".\",  # save output in current directory\n",
    "    expire_time=60 * 3,  # set all process data to expire in 5 minutes\n",
    "    wait_for_process=True,  # wait for process to complete before regaining ide\n",
    "    verbose=False,\n",
    ")  # set verbosity to False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of this process is printed below.  Because the output of this particular module-model pair is a faiss database, the process output is provided in this object is null.  However the file itself has been returned to the address noted in the `process_output_files` key.  The `file_id` of the processed input is used as a filename prefix for the output file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'json_print' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# nicely print the output of this process\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mjson_print\u001b[49m(process_output)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'json_print' is not defined"
     ]
    }
   ],
   "source": [
    "# nicely print the output of this process\n",
    "json_print(process_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying output databases locally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now perform queries on the pulled vector database whose location is given in `process_output_files`.\n",
    "\n",
    "Below is a simple function for performing single keyword queries on this database locally.  Note: you will need to install the faiss library to execute this cell.  Install [faiss-cpu](https://pypi.org/project/faiss-cpu/) or [faiss-gpu](https://pypi.org/project/faiss-gpu/) depending on the specs of your local setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "from typing import Tuple\n",
    "\n",
    "\n",
    "def query_vector_db(\n",
    "    query_vector: np.ndarray, k: int, db_file_path: str\n",
    ") -> Tuple[list, list]:\n",
    "    # read in vector db\n",
    "    faiss_index = faiss.read_index(db_file_path)\n",
    "\n",
    "    # perform query\n",
    "    similarities, indices = faiss_index.search(query_vector, k)\n",
    "    distances = 1 - similarities\n",
    "    return distances, indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform a simple query using the test function above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input query vector: [0 1]\n",
      "closest vector from original: [0 1]\n",
      "distance from query to this vector: 0.0\n",
      "second closest vector from original: [1 1]\n",
      "distance from query to this vector: 0.2928932309150696\n"
     ]
    }
   ],
   "source": [
    "# perform test query using the above query function\n",
    "original_vectors = np.load(test_file)\n",
    "query_vector = np.array([[0, 1]])\n",
    "distances, indices = query_vector_db(\n",
    "    query_vector, 2, process_output[\"process_output_files\"][0]\n",
    ")\n",
    "print(f\"input query vector: {query_vector[0]}\")\n",
    "print(f\"closest vector from original: {original_vectors[indices[0][0]]}\")\n",
    "print(f\"distance from query to this vector: {distances[0][0]}\")\n",
    "print(f\"second closest vector from original: {original_vectors[indices[0][1]]}\")\n",
    "print(f\"distance from query to this vector: {distances[0][1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the `vector_search` method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "krixik's `vector_search` method is a convenience function for both embedding and querying - and so can only be used with pipelines containing both `text-embedder` and `vector-search` modules in succession.\n",
    "\n",
    "Below we construct the simplest custom pipeline that satisfies this criteria - a standard vector search pipeline consisting of three modules: a `parser`, `text-embedder`, and `vector-search` index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import custom module creation tools\n",
    "from krixik.pipeline_builder.module import Module\n",
    "from krixik.pipeline_builder.pipeline import CreatePipeline\n",
    "\n",
    "# instantiate module\n",
    "module_1 = Module(module_type=\"parser\")\n",
    "module_2 = Module(module_type=\"text-embedder\")\n",
    "module_3 = Module(module_type=\"vector-db\")\n",
    "\n",
    "# create custom pipeline object\n",
    "custom = CreatePipeline(\n",
    "    name=\"vector-search-pipeline-1\", module_chain=[module_1, module_2, module_3]\n",
    ")\n",
    "\n",
    "# pass the custom object to the krixik operator (note you can also do this by passing its config)\n",
    "pipeline = krixik.load_pipeline(pipeline=custom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": [
     "remove_convert"
    ]
   },
   "outputs": [],
   "source": [
    "reset_pipeline(pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now perform any of the core system methods on our custom pipeline (e.g., `.process`, `.list`, etc.,).  Additionally we can invoke the `vector_search` method.\n",
    "\n",
    "Lets first process a file with our new pipeline.  The `vector-search` module takes in a text file, and returns `faiss` vector database consisting of all non-trivial `(snippet, line_numbers)` tuples from the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: hydrated input modules: {'module_1': {'model': 'sentence', 'params': {}}, 'module_2': {'model': 'all-MiniLM-L6-v2', 'params': {'quantize': True}}, 'module_3': {'model': 'faiss', 'params': {}}}\n",
      "INFO: symbolic_directory_path was not set by user - setting to default of /etc\n",
      "INFO: file_name was not set by user - setting to random file name: krixik_generated_file_name_pnzmdtxamt.txt\n",
      "INFO: wait_for_process is set to True.\n",
      "INFO: file will expire and be removed from you account in 600 seconds, at Thu May  2 10:23:50 2024 UTC\n",
      "INFO: vector-search-pipeline-1 file process and input processing started...\n",
      "INFO: metadata can be updated using the .update api.\n",
      "INFO: This process's request_id is: b1461aaf-cf57-5980-a3c3-29bc7ec6b204\n",
      "INFO: File process and processing status:\n",
      "SUCCESS: module 1 (of 3) - parser processing complete.\n",
      "SUCCESS: module 2 (of 3) - text-embedder processing complete.\n",
      "SUCCESS: module 3 (of 3) - vector-db processing complete.\n",
      "SUCCESS: pipeline process complete.\n",
      "SUCCESS: process output downloaded\n",
      "{\n",
      "  \"status_code\": 200,\n",
      "  \"pipeline\": \"vector-search-pipeline-1\",\n",
      "  \"request_id\": \"f3dce414-a9a9-46f5-8168-e7eead71989f\",\n",
      "  \"file_id\": \"fe920974-67b6-4471-873d-e7b75a28854e\",\n",
      "  \"message\": \"SUCCESS - output fetched for file_id fe920974-67b6-4471-873d-e7b75a28854e.Output saved to location(s) listed in process_output_files.\",\n",
      "  \"warnings\": [],\n",
      "  \"process_output\": null,\n",
      "  \"process_output_files\": [\n",
      "    \"./fe920974-67b6-4471-873d-e7b75a28854e.faiss\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# define path to an input file from examples directory\n",
    "test_file = \"../input_data/1984_very_short.txt\"\n",
    "\n",
    "# process for search\n",
    "process_output = pipeline.process(\n",
    "    local_file_path=test_file,\n",
    "    local_save_directory=\".\",  # save output in current directory\n",
    "    expire_time=60 * 10,  # set all process data to expire in 5 minutes\n",
    "    wait_for_process=True,  # wait for process to complete before regaining ide\n",
    "    verbose=True,\n",
    ")  # set verbosity to False\n",
    "\n",
    "# nicely print the output of this process\n",
    "json_print(process_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can query our text with natural language as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"status_code\": 200,\n",
      "  \"request_id\": \"98e6c488-440d-4a1a-978b-bb82affcb1b4\",\n",
      "  \"message\": \"Successfully queried 1 user file.\",\n",
      "  \"warnings\": [],\n",
      "  \"items\": [\n",
      "    {\n",
      "      \"file_id\": \"656c6486-8a89-43ea-8658-762dbf8b9c9c\",\n",
      "      \"file_metadata\": {\n",
      "        \"file_name\": \"krixik_generated_file_name_advielayge.txt\",\n",
      "        \"symbolic_directory_path\": \"/etc\",\n",
      "        \"file_tags\": [],\n",
      "        \"num_vectors\": 2,\n",
      "        \"created_at\": \"2024-04-28 16:21:06\",\n",
      "        \"last_updated\": \"2024-04-28 16:21:06\"\n",
      "      },\n",
      "      \"search_results\": [\n",
      "        {\n",
      "          \"snippet\": \"It was a bright cold day in April, and the clocks were striking thirteen.\",\n",
      "          \"line_numbers\": [\n",
      "            1\n",
      "          ],\n",
      "          \"distance\": 0.224\n",
      "        },\n",
      "        {\n",
      "          \"snippet\": \"Winston Smith, his chin nuzzled into his breast in an effort to escape the\\nvile wind, slipped quickly through the glass doors of Victory Mansions,\\nthough not quickly enough to prevent a swirl of gritty dust from entering\\nalong with him.\",\n",
      "          \"line_numbers\": [\n",
      "            2,\n",
      "            3,\n",
      "            4,\n",
      "            5\n",
      "          ],\n",
      "          \"distance\": 0.417\n",
      "        }\n",
      "      ]\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# perform vector_search over the input file\n",
    "vector_output = pipeline.vector_search(\n",
    "    query=\"it was cold night\", file_ids=[process_output[\"file_id\"]]\n",
    ")\n",
    "\n",
    "# nicely print the output of this process\n",
    "json_print(vector_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "remove_convert"
    ]
   },
   "outputs": [],
   "source": [
    "# delete all processed datapoints belonging to this pipeline\n",
    "reset_pipeline(pipeline)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
