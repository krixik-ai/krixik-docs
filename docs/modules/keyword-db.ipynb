{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The `keyword-db` module\n",
    "\n",
    "This document reviews the `keyword-db` module - which takes as input a document, parses the documents for non-trivial keywords and their lemmatized stems, and returns a database with this content.\n",
    "\n",
    "This document includes an overview of custom pipeline setup, current model set, parameters, and `.process` usage for this module.\n",
    "\n",
    "To follow along with this demonstration be sure to initialize your krixik session with your api key and url as shown below. \n",
    "\n",
    "We illustrate loading these required secrets in via [python-dotenv](https://pypi.org/project/python-dotenv/), storing those secrets in a `.env` file.  This is always good practice for storing / loading secrets (e.g., doing so will reduce the chance you inadvertantly push secrets to a repo)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append('../../')\n",
    "import importlib\n",
    "reset = importlib.import_module(\"utilities.reset\")\n",
    "reset_pipeline = reset.reset_pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "remove_output"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCESS: You are now authenticated.\n"
     ]
    }
   ],
   "source": [
    "# load secrets from a .env file using python-dotenv\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv(\"../../.env\")\n",
    "MY_API_KEY = os.getenv('MY_API_KEY')\n",
    "MY_API_URL = os.getenv('MY_API_URL')\n",
    "\n",
    "# import krixik and initialize it with your personal secrets\n",
    "from krixik import krixik\n",
    "krixik.init(api_key = MY_API_KEY, \n",
    "            api_url = MY_API_URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This small function prints dictionaries very nicely in notebooks / markdown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print dictionaries / json nicely in notebooks / markdown\n",
    "import json\n",
    "def json_print(data):\n",
    "    print(json.dumps(data, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A table of contents for the remainder of this document is shown below.\n",
    "\n",
    "\n",
    "- [pipeline setup](#pipeline-setup)\n",
    "- [required input format](#required-input-format)\n",
    "- [using the default model](#using-the-default-model)\n",
    "- [using the `keyword_search` method](#using-the-keyword-search-method)\n",
    "- [querying output databases locally](#querying-output-databases-locally)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline setup\n",
    "\n",
    "Below we setup a simple one module pipeline using the `keyword-search` module. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a pipeline with a single module\n",
    "pipeline = krixik.create_pipeline(name=\"my-keyword-db-pipeline\",\n",
    "                                  module_chain=[\"keyword-db\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `keyword-search` module comes with a single model:\n",
    "\n",
    "- `base`: (default) parses input document for non-trivial keywords\n",
    "\n",
    "These available modeling options and parameters are stored in our custom pipeline's configuration (described further in LINK HERE).  We can examine this configuration as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"pipeline\": {\n",
      "    \"name\": \"my-keyword-db-pipeline\",\n",
      "    \"modules\": [\n",
      "      {\n",
      "        \"name\": \"keyword-db\",\n",
      "        \"models\": [\n",
      "          {\n",
      "            \"name\": \"sqlite\"\n",
      "          }\n",
      "        ],\n",
      "        \"defaults\": {\n",
      "          \"model\": \"sqlite\"\n",
      "        },\n",
      "        \"input\": {\n",
      "          \"type\": \"text\",\n",
      "          \"permitted_extensions\": [\n",
      "            \".txt\",\n",
      "            \".pdf\",\n",
      "            \".docx\",\n",
      "            \".pptx\"\n",
      "          ]\n",
      "        },\n",
      "        \"output\": {\n",
      "          \"type\": \"db\"\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# nicely print pipeline configuration\n",
    "json_print(pipeline.config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see the models and their associated parameters available for use.\n",
    "\n",
    "You can save this configuration to disk as well by executing\n",
    "\n",
    "\n",
    "```python\n",
    "pipeline.save_pipeline(\"/valid/path/file.yml\")\n",
    "```\n",
    "\n",
    "You can instantiate a pipeline directly from its configuration using the [.load_pipeline method](LINK HERE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# delete all processed datapoints belonging to this pipeline\n",
    "reset_pipeline(pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required input format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `keyword-db` module accepts `.txt`, `.pdf`, `.docx`, and `.pptx` file formats as input.  The latter three (`.pdf`, `.docx`, and `.pptx`) are first converted to `.txt` prior to processing.\n",
    "\n",
    "Let's look at an example of a small valid input - and then process it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It was a bright cold day in April, and the clocks were striking thirteen.\n",
      "Winston Smith, his chin nuzzled into his breast in an effort to escape the\n",
      "vile wind, slipped quickly through the glass doors of Victory Mansions,\n",
      "though not quickly enough to prevent a swirl of gritty dust from entering\n",
      "along with him.\n"
     ]
    }
   ],
   "source": [
    "# examine contents of a valid test input file\n",
    "test_file = \"../../data/input/1984_very_short.txt\"\n",
    "with open(test_file, \"r\") as file:\n",
    "    print(file.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the default model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's process it using our default model - `base`.  Because `base` is the default model we need not input the optional `modules` argument into `.process`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define path to an input file from examples directory\n",
    "test_file = \"../../data/input/1984_very_short.txt\"\n",
    "\n",
    "# process for search\n",
    "process_output = pipeline.process(local_file_path = test_file,\n",
    "                                  local_save_directory=\"../../data/output\", # save output repo data output subdir\n",
    "                                  expire_time=60 * 10,    # set all process data to expire in 10 minutes\n",
    "                                  wait_for_process=True,    # wait for process to complete before regaining ide\n",
    "                                  verbose=False)            # set verbosity to False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of this process is printed below.  Because the output of this particular module-model pair is a sqlite database, the process output is provided in this object is null.  However the file itself has been returned to the address noted in the `process_output_files` key.  The `file_id` of the processed input is used as a filename prefix for the output file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"status_code\": 200,\n",
      "  \"pipeline\": \"my-keyword-db-pipeline\",\n",
      "  \"request_id\": \"5951c05e-5cb0-4c17-8d4d-d87c887c9cd1\",\n",
      "  \"file_id\": \"36c032ab-ac57-4c3f-a67a-0810fee184bd\",\n",
      "  \"message\": \"SUCCESS - output fetched for file_id 36c032ab-ac57-4c3f-a67a-0810fee184bd.Output saved to location(s) listed in process_output_files.\",\n",
      "  \"warnings\": [],\n",
      "  \"process_output\": null,\n",
      "  \"process_output_files\": [\n",
      "    \"../../data/output/36c032ab-ac57-4c3f-a67a-0810fee184bd.db\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# nicely print the output of this process\n",
    "json_print(process_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the `keyword_search` method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any pipeline containing a `keyword-search` module automatically inherits access to the [`keyword_search` method](keyword_search_method.md).  This provides convenient sophisticated query access to the newly created keyword database in krixik.\n",
    "\n",
    "The `keyword_search` method takes in an input `query` containing desired keywords separated by spaces, and searches through your database(s) for these keywords as well as their lemmatized stems.\n",
    "\n",
    "An example use if given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"status_code\": 200,\n",
      "  \"request_id\": \"03d35d92-edf9-4969-8b5a-481606535b09\",\n",
      "  \"message\": \"Successfully queried 1 user file.\",\n",
      "  \"warnings\": [\n",
      "    {\n",
      "      \"WARNING: the following words in the query are in the stop_words list and thus no results will be returned for them\": [\n",
      "        \"it\",\n",
      "        \"was\"\n",
      "      ]\n",
      "    }\n",
      "  ],\n",
      "  \"items\": [\n",
      "    {\n",
      "      \"file_id\": \"36c032ab-ac57-4c3f-a67a-0810fee184bd\",\n",
      "      \"file_metadata\": {\n",
      "        \"file_name\": \"krixik_generated_file_name_sblojcvlzc.txt\",\n",
      "        \"symbolic_directory_path\": \"/etc\",\n",
      "        \"file_tags\": [],\n",
      "        \"num_lines\": 5,\n",
      "        \"created_at\": \"2024-05-03 21:59:13\",\n",
      "        \"last_updated\": \"2024-05-03 21:59:13\"\n",
      "      },\n",
      "      \"search_results\": [\n",
      "        {\n",
      "          \"keyword\": \"cold\",\n",
      "          \"line_number\": 1,\n",
      "          \"keyword_number\": 5\n",
      "        }\n",
      "      ]\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# perform keyword_search over the input file\n",
    "keyword_output = pipeline.keyword_search(query=\"it was cold night\",\n",
    "                                         file_ids=[process_output[\"file_id\"]])\n",
    "\n",
    "# nicely print the output of this process\n",
    "json_print(keyword_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying output databases locally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now perform queries on the pulled keyword database whose location is given in `process_output_files`.\n",
    "\n",
    "Below is a simple function for performing single keyword queries on this database locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "def query_db(query_keyword: str,\n",
    "             keyword_db_local_file_name: str) -> list:\n",
    "    # load keyword_db\n",
    "    keyword_db = sqlite3.connect(keyword_db_local_file_name)\n",
    "    keyword_cursor = keyword_db.cursor()\n",
    "    \n",
    "    # create query pattern\n",
    "    query_pattern = f\"\"\"\n",
    "    SELECT\n",
    "        original_keyword,\n",
    "        line_number,\n",
    "        keyword_number\n",
    "    FROM\n",
    "        keyword_search\n",
    "    where original_keyword=\"{query_keyword}\"\n",
    "    GROUP BY\n",
    "        original_keyword,\n",
    "        line_number,\n",
    "        keyword_number\n",
    "    ORDER BY\n",
    "        line_number,\n",
    "        keyword_number\n",
    "    \"\"\"\n",
    "    \n",
    "    # excute query\n",
    "    keyword_cursor.execute(query_pattern)\n",
    "\n",
    "    # Fetch and process the results\n",
    "    rows = keyword_cursor.fetchall()\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we query our small database using a single keyword query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cold', 1, 5)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# query database\n",
    "query = \"cold\"\n",
    "query_db(query,\n",
    "         process_output['process_output_files'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# delete all processed datapoints belonging to this pipeline\n",
    "reset_pipeline(pipeline)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
